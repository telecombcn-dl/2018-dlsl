---
title: "Lectures"
bg: #9AD1F5
color: black
fa-icon: clock-o
---

## Lectures (40%) - Room: D5-010

Course will be divided in modules of half an hour covering the following topics:

* 22/01 10:00 D1L1 (XG) Welcome  
* 22/01 10:30 D1L2 (VV) Machine Learning
* 22/01 11:00 D1L3 (AB) Perceptron
* 22/01 11:30 D1L4 (ES) Multi-Layer Perceptron
* 23/01 10:00 D2L1 (ES) Backpropagation
* 23/01 10:30 D2L2 (VV) Optimizers
* 23/01 11:00 D2L3 (JR) Loss functions 
* 23/01 11:30 D2L4 (VV) Convolutional Neural Networks - CNNs
* 24/01 10:00 D3L1 (RM) Transfer learning
* 24/01 10:30 D3L2 (MRC) Recurrent Neural Networks - RNNs
* 24/01 11:00 D3L3 (MRC) Gated RNNs
* 24/01 11:30 D3L4 (MRC) Attention Models
* 25/01 10:00 D4L1 (XG) Unsupervised learning
* 25/01 10:30 D4L2 (VV) Generative models
* 25/01 11:00 D4L3 (XG) Reinforcement learning - RL
* 25/01 11:30 D4L4 (XG) Architectures 
* 30/01 Guest [Jordi Torres][JordiTorres] (Barcelona Supercomputing Center) 

[JordiTorres]: http://jorditorres.org/

## Labs (20%) - Rooms: D5-004, D5-005 & D5-007
The course will contain guided hands on lab provided by the NVIDIA Deep Learning Institute.

* 22/01 [Lab1: Linear Classification with Tensorflow][Lab3]
* 23/01 [Lab2: Image Classification with DIGITS][Lab1] 
* 24/01 [Lab3: Modeling Time Series Data with Recurrent Neural Networks in Keras][Lab2]
* 25/01 [Lab4: Identifying Whale Sounds with Audio Classification][Lab4]

[Lab1]: https://nvidia.qwiklab.com/focuses/3045
[Lab2]: https://nvidia.qwiklab.com/focuses/5866
[Lab3]: https://nvidia.qwiklab.com/focuses/3043
[Lab4]: https://nvidia.qwiklab.com/focuses/5451


## Project (40%) - Rooms: D5-004, D5-005 & D5-007

Students will work in teams to develop a machine learning research project that will be presented in an oral presentation during the final day of the course. 

* 22/01: Getting started 
* 23/01: First steps with [Keras](https://keras.io/) and [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)(JT)
* 24/01: Multi-layer Perceptron
* 25/01: Convolutional Neural Networks
